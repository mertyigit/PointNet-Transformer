{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visual Transformer 2D**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import kl_div\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import open3d as o3\n",
    "import math\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.models.VisualTransformerEncoder import *\n",
    "from src.models.VisualTransformerDecoder import *\n",
    "from src.models.MultiHeadAttentionBlock import *\n",
    "from src.models.VisualTransformerGenerator import *\n",
    "from src.utils import features, utils\n",
    "from src.data.dataset import DataMNIST\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP for supressing pytorch user warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is build: True\n",
      "MPS Availability: True\n",
      "Device is set to :mps\n"
     ]
    }
   ],
   "source": [
    "print('MPS is build: {}'.format(torch.backends.mps.is_built()))\n",
    "print('MPS Availability: {}'.format(torch.backends.mps.is_available()))\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print('Device is set to :{}'.format(DEVICE))\n",
    "torch.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataMNIST(**config[\"data_parameters\"])\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.train_dataloader()\n",
    "val_dataloader = data.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "60000it [00:17, 3361.70it/s]\n"
     ]
    }
   ],
   "source": [
    "### GENERATE DATA ###\n",
    "training_data = []\n",
    "for index_ref, (image_ref, number_ref) in tqdm(enumerate(data.train_dataset)):\n",
    "    for index_gen, (image_gen, number_gen) in enumerate(data.train_dataset):\n",
    "        if  index_ref != index_gen and number_ref == number_gen:\n",
    "            training_data.append((image_ref, image_gen))\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:03, 2826.10it/s]\n"
     ]
    }
   ],
   "source": [
    "### GENERATE VALIDATION DATA ###\n",
    "validating_data = []\n",
    "for index_ref, (image_ref, number_ref) in tqdm(enumerate(data.val_dataset)):\n",
    "    for index_gen, (image_gen, number_gen) in enumerate(data.val_dataset):\n",
    "        if  index_ref != index_gen and number_ref == number_gen:\n",
    "            validating_data.append((image_ref, image_gen))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(\n",
    "            validating_data,\n",
    "            batch_size=16,\n",
    "            shuffle=True,\n",
    "            pin_memory=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE & POST PROCESSING SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TensorToImageGrid(images_batch, rows, cols):\n",
    "    grid = torchvision.utils.make_grid(images_batch, nrow=cols)\n",
    "    grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(cols, rows))\n",
    "    plt.imshow(grid, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    return plt.show()\n",
    "\n",
    "def TensorToImage(image):\n",
    "    plt.figure(figsize = (2, 2))\n",
    "    plt.imshow(image.numpy(), cmap='gray')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [20:36<00:00,  3.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1; Loss: 0.032163921030486625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3750/3750 [20:30<00:00,  3.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2; Loss: 0.006669528393726796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_patches = 7\n",
    "num_layers = 4\n",
    "hidden_d = 128\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.0001\n",
    "\n",
    "transformer = Transformer(hidden_d, n_heads, num_layers, d_ff, dropout, n_patches).to(DEVICE)\n",
    "criterion = nn.MSELoss().to(DEVICE)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate, eps=1e-9)\n",
    "transformer.train()\n",
    "\n",
    "\n",
    "for epoch in range(2):\n",
    "    loss_epoch = []\n",
    "    n=0\n",
    "    for images, y in tqdm(train_dataloader):\n",
    "        #images, y = batch\n",
    "        #print(batch)\n",
    "        images = images.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(images)\n",
    "        loss = criterion(y, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_epoch.append(loss.detach().cpu().item()) \n",
    "        #torch.mps.empty_cache()\n",
    "    \n",
    "    print(\"Epoch: {}; Loss: {}\".format(epoch+1, np.mean(loss_epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.detach().cpu().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_ref, images_gen = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "N=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 16 is out of bounds for dimension 0 with size 16",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/mertyigitsengul/Projects/PointNet_Transformer/notebooks/vit_dev.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/mertyigitsengul/Projects/PointNet_Transformer/notebooks/vit_dev.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m TensorToImage(transformer(images_ref)[N]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()), TensorToImage(images_gen[N]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()), TensorToImage(images_ref[N]\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu())\n",
      "\u001b[0;31mIndexError\u001b[0m: index 16 is out of bounds for dimension 0 with size 16"
     ]
    }
   ],
   "source": [
    "TensorToImage(transformer(images_ref)[N].squeeze().detach().cpu()), TensorToImage(images_gen[N].squeeze().detach().cpu()), TensorToImage(images_ref[N].squeeze().detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3515861ec4313dacaa20b0eec5bf326e6557b6589b7b6a4fe3c8baa566747d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
