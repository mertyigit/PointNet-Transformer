{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visual Transformer 2D**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import kl_div\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import open3d as o3\n",
    "import math\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "#from open3d.web_visualizer import draw # for non Colab\n",
    "#sys.path.append('../../PointNet-VAE/src')\n",
    "sys.path.append('../src')\n",
    "\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP for supressing pytorch user warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is build: True\n",
      "MPS Availability: True\n",
      "Device is set to :mps\n"
     ]
    }
   ],
   "source": [
    "print('MPS is build: {}'.format(torch.backends.mps.is_built()))\n",
    "print('MPS Availability: {}'.format(torch.backends.mps.is_available()))\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print('Device is set to :{}'.format(DEVICE))\n",
    "torch.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = ToTensor()\n",
    "\n",
    "train_set = MNIST(\n",
    "    root=\"./../data\", train=True, download=True, transform=transform\n",
    ")\n",
    "test_set = MNIST(\n",
    "    root=\"./../data\", train=False, download=True, transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_set, shuffle=True, batch_size=128)\n",
    "test_loader = DataLoader(test_set, shuffle=False, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE & POST PROCESSING SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def patchify(images, n_patches):\n",
    "    n, c, h, w = images.shape\n",
    "\n",
    "    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "\n",
    "    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches**2)\n",
    "    patch_size = h // n_patches\n",
    "\n",
    "    for idx, image in enumerate(images):\n",
    "        for i in range(n_patches):\n",
    "            for j in range(n_patches):\n",
    "                patch = image[\n",
    "                    :,\n",
    "                    i * patch_size : (i + 1) * patch_size,\n",
    "                    j * patch_size : (j + 1) * patch_size,\n",
    "                ]\n",
    "                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "    return patches\n",
    "\n",
    "def depatchify(patches, n_patches, chw):\n",
    "    # size of patches is expected to be n, h, w\n",
    "    patch_h = chw[1] // n_patches\n",
    "    patch_w = chw[2] // n_patches\n",
    "    n = patches.shape[0]\n",
    "\n",
    "    images_recovered = torch.zeros(n, chw[0], chw[1], chw[2],  device='mps')\n",
    "\n",
    "    for idx, patch in enumerate(patches):\n",
    "        \n",
    "        image_r = torch.empty(0, chw[2],  device='mps')\n",
    "        \n",
    "        for i in range(n_patches):\n",
    "            #patch_r_i_1 = patch[i, :].view(patch_h, patch_w)\n",
    "            row_tensor = torch.empty(patch_h, 0,  device='mps')\n",
    "            for j in range(n_patches):\n",
    "                patch_r_row = patch[i*n_patches+j, :].view(patch_h, patch_w)\n",
    "                row_tensor = torch.cat((row_tensor, patch_r_row), dim=1)\n",
    "            \n",
    "            image_r = torch.cat((image_r, row_tensor), dim=0)\n",
    "\n",
    "        images_recovered[idx] = image_r\n",
    "    return images_recovered\n",
    "\n",
    "def get_positional_embeddings(sequence_length, d):\n",
    "    result = torch.ones(sequence_length, d)\n",
    "    for i in range(sequence_length):\n",
    "        for j in range(d):\n",
    "            result[i][j] = (\n",
    "                np.sin(i / (10000 ** (j / d)))\n",
    "                if j % 2 == 0\n",
    "                else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "            )\n",
    "    return result\n",
    "\n",
    "def TensorToImageGrid(images_batch, rows, cols):\n",
    "    grid = torchvision.utils.make_grid(images_batch, nrow=cols)\n",
    "    grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(cols, rows))\n",
    "    plt.imshow(grid, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    return plt.show()\n",
    "\n",
    "def TensorToImage(image):\n",
    "    plt.imshow(image.numpy(), cmap='gray')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class SelfAttentionLayer(nn.Module):\n",
    "#    '''\n",
    "#    hidden_d: number of hidden dimensions\n",
    "#    n_heads: number of attention heads\n",
    "#    \n",
    "#    '''\n",
    "#    def __init__(self, hidden_d, n_heads=2):\n",
    "#        super(SelfAttentionLayer, self).__init__()\n",
    "#        self.hidden_d = hidden_d\n",
    "#        self.n_heads = n_heads\n",
    "#\n",
    "#        assert hidden_d % n_heads == 0, f\"{hidden_d} cannot be divided into {n_heads} heads!\"\n",
    "#\n",
    "#        d_head = int(hidden_d / n_heads) ## Hidded dimensions per head\n",
    "#        \n",
    "#        # Define the linear transformation layers for Query, Key, and Value\n",
    "#        self.q_mappings = nn.ModuleList(\n",
    "#            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "#        )\n",
    "#        self.k_mappings = nn.ModuleList(\n",
    "#            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "#        )\n",
    "#        self.v_mappings = nn.ModuleList(\n",
    "#            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "#        )\n",
    "#        self.d_head = d_head\n",
    "#        self.softmax = nn.Softmax(dim=-1)\n",
    "#\n",
    "#    def forward(self, sequences):\n",
    "#        \n",
    "#        # Sequences has shape (N, seq_length, token_dim)\n",
    "#        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "#        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "#\n",
    "#        result = []\n",
    "#        for sequence in sequences:\n",
    "#            seq_result = []\n",
    "#            for head in range(self.n_heads):\n",
    "#                q_mapping = self.q_mappings[head]\n",
    "#                k_mapping = self.k_mappings[head]\n",
    "#                v_mapping = self.v_mappings[head]\n",
    "#\n",
    "#                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
    "#                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "#\n",
    "#                attention = self.softmax(q @ k.T / (self.d_head**0.5)) # Apply the softmax function to calculated the scaled dot-product attention scores to get attention weights\n",
    "#                # attention = F.dropout(attention, p=0.1) # Drop out to attention - optional\n",
    "#                seq_result.append(attention @ v)  # Calculate the weighted sum using the attention weights\n",
    "#            result.append(torch.hstack(seq_result))\n",
    "#        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert hidden_d % n_heads == 0, f\"{hidden_d} cannot be divided into {n_heads} heads!\"\n",
    "\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = hidden_d // n_heads ## Hidded dimensions per head\n",
    "        \n",
    "        # Define the linear transformation layers for Query, Key, and Value\n",
    "        self.W_q = nn.Linear(hidden_d, hidden_d)\n",
    "        self.W_k = nn.Linear(hidden_d, hidden_d)\n",
    "        self.W_v = nn.Linear(hidden_d, hidden_d)\n",
    "        self.W_o = nn.Linear(hidden_d, hidden_d)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        # Apply the softmax function to calculated the scaled dot-product attention scores to get attention weights\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "        output = torch.matmul(attn_probs, V) # Calculate the weighted sum using the attention weights\n",
    "        return output\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        batch_size, seq_length, hidden_d = x.size()\n",
    "        return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        batch_size, _, seq_length, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_d)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        \n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_d, d_ff):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(hidden_d, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, hidden_d)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class VisualTransformerEncoderBlock(nn.Module):\n",
    "    '''\n",
    "    Not typical transformer block, the normalization and linear layers are there but the order is different.\n",
    "    '''\n",
    "    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "        super(VisualTransformerEncoderBlock, self).__init__()\n",
    "        self.hidden_d = hidden_d\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.mhsa = MultiHeadAttention(hidden_d, n_heads)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x + self.mhsa(self.norm1(x), self.norm1(x), self.norm1(x)) \n",
    "        out = out + self.mlp(self.norm2(out))\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformerEncoder(nn.Module):\n",
    "    def __init__(self, chw, n_patches=7, num_layers=2, hidden_d=8, n_heads=2):\n",
    "        # Super constructor\n",
    "        super(VisualTransformerEncoder, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.chw = chw  # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.num_layers = num_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "\n",
    "        # Input and patches sizes\n",
    "        assert (\n",
    "            chw[1] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "        assert (\n",
    "            chw[2] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        self.register_buffer(\n",
    "            \"positional_embeddings\",\n",
    "            get_positional_embeddings(n_patches**2, hidden_d),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [VisualTransformerEncoderBlock(hidden_d, n_heads) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, images):\n",
    "        # Image to Patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Patch vector to Hidden Dimensions\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        #tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "\n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out)\n",
    "\n",
    "        return out, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformerDecoderBlock(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, d_ff, dropout):\n",
    "        super(VisualTransformerDecoderBlock, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(hidden_d, n_heads)\n",
    "        self.cross_attn = MultiHeadAttention(hidden_d, n_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(hidden_d, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "        self.norm3 = nn.LayerNorm(hidden_d)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisualTransformerDecoder(nn.Module):\n",
    "    def __init__(self, chw, d_ff, dropout, n_patches=7, num_layers=2, hidden_d=8, n_heads=2):\n",
    "        # Super constructor\n",
    "        super(VisualTransformerDecoder, self).__init__()\n",
    "\n",
    "        # Attributes\n",
    "        self.chw = chw  # ( C , H , W )\n",
    "        self.n_patches = n_patches\n",
    "        self.num_layers = num_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        # Input and patches sizes\n",
    "        assert (\n",
    "            chw[1] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "        assert (\n",
    "            chw[2] % n_patches == 0\n",
    "        ), \"Input shape not entirely divisible by number of patches\"\n",
    "        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "\n",
    "        # 1) Linear mapper\n",
    "        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "\n",
    "        # 2) Learnable classification token\n",
    "        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "\n",
    "        # 3) Positional embedding\n",
    "        self.register_buffer(\n",
    "            \"positional_embeddings\",\n",
    "            get_positional_embeddings(n_patches**2, hidden_d),\n",
    "            persistent=False,\n",
    "        )\n",
    "\n",
    "        # 4) Transformer encoder blocks\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [VisualTransformerDecoderBlock(hidden_d, n_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        self.linear_decoder = nn.Linear(self.hidden_d, self.input_d)\n",
    "\n",
    "    def forward(self, images,  enc_output, src_mask, tgt_mask):\n",
    "        # Image to Patches\n",
    "        n, c, h, w = images.shape\n",
    "        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "\n",
    "        # Patch vector to Hidden Dimensions\n",
    "        tokens = self.linear_mapper(patches)\n",
    "\n",
    "        # Adding classification token to the tokens\n",
    "        #tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "\n",
    "        # Adding positional embedding\n",
    "        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "\n",
    "        # Transformer Blocks\n",
    "        for block in self.blocks:\n",
    "            out = block(out, enc_output, src_mask, tgt_mask)\n",
    "        \n",
    "        out = self.linear_decoder(out)\n",
    "\n",
    "        images = depatchify(out, self.n_patches, self.chw).to(self.positional_embeddings.device)\n",
    "\n",
    "        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, hidden_d, n_heads, num_layers, d_ff, dropout, n_patches):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.hidden_d = hidden_d\n",
    "        self.num_layers = num_layers\n",
    "        self.d_ff = d_ff\n",
    "        self.dropout = dropout\n",
    "        self.n_patches = n_patches\n",
    "\n",
    "        self.encoder = VisualTransformerEncoder((1, 28, 28), n_patches=self.n_patches, num_layers=self.num_layers, hidden_d=self.hidden_d, n_heads=self.n_heads)\n",
    "        self.decoder = VisualTransformerDecoder((1, 28, 28), d_ff=self.d_ff, dropout=self.dropout, n_patches=self.n_patches, num_layers=self.num_layers, hidden_d=self.hidden_d, n_heads=self.n_heads)\n",
    "\n",
    "        \n",
    "    def generate_mask(self, patches):\n",
    "        src = torch.ones((patches.size(0), patches.size(1)))\n",
    "        tgt = torch.ones((patches.size(0), patches.size(1)))\n",
    "\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, images):\n",
    "        enc_output, patches = self.encoder(images)\n",
    "        src_mask, tgt_mask = self.generate_mask(patches)\n",
    "        src_mask = src_mask.to(device='mps')\n",
    "        tgt_mask = tgt_mask.to(device='mps')\n",
    "        dec_output = self.decoder(images, enc_output, src_mask, tgt_mask)\n",
    "        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patches = 7\n",
    "num_layers = 2\n",
    "hidden_d = 64\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "\n",
    "\n",
    "encoder = VisualTransformerEncoder((1, 28, 28), n_patches=n_patches, num_layers=num_layers, hidden_d=hidden_d, n_heads=n_heads).to(DEVICE)\n",
    "##decoder_layers = nn.ModuleList([VisualTransformerDecoderBlock(hidden_d, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "decoder = VisualTransformerDecoder((1, 28, 28), d_ff=d_ff, dropout=dropout, n_patches=n_patches, num_layers=num_layers, hidden_d=hidden_d, n_heads=n_heads).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, classes = next(iter(train_loader))\n",
    "print(images.shape, classes.shape)\n",
    "patches = patchify(images, n_patches).unsqueeze(1)\n",
    "print(patches.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mask(patches):\n",
    "    src = torch.ones((patches.size(0), patches.size(2)))\n",
    "    tgt = torch.ones((patches.size(0), patches.size(2)))\n",
    "\n",
    "    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "    seq_length = tgt.size(1)\n",
    "    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "    tgt_mask = tgt_mask & nopeak_mask\n",
    "    return src_mask, tgt_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_mask, tgt_mask = generate_mask(patches)\n",
    "src_mask = src_mask.to(DEVICE)\n",
    "tgt_mask = tgt_mask.to(DEVICE)\n",
    "#src_embedded = positional_encoding(encoder_embedding(src))\n",
    "#tgt_embedded = positional_encoding(decoder_embedding(tgt))\n",
    "#enc_output = src_embedded\n",
    "enc_output = encoder(images)\n",
    "print('Encoded Latent Shape: {}'.format(enc_output.shape))\n",
    "dec_output = decoder(images, enc_output, src_mask, tgt_mask)\n",
    "print('Decoded Sequence Shape: {}'.format(dec_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/469 [00:05<39:22,  5.05s/it]"
     ]
    }
   ],
   "source": [
    "n_patches = 7\n",
    "num_layers = 2\n",
    "hidden_d = 64\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(hidden_d, n_heads, num_layers, d_ff, dropout, n_patches).to(DEVICE)\n",
    "#criterion = nn.CrossEntropyLoss(ignore_index=0).to(DEVICE)\n",
    "criterion = nn.MSELoss().to(DEVICE)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(5):\n",
    "    for batch in tqdm(train_loader):\n",
    "        images, _ = batch\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(images)\n",
    "        loss = criterion(images, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "transformer.eval()\n",
    "\n",
    "TensorToImage(transformer(images)[1].squeeze().detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TensorToImage(images[1].squeeze().detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorToImage(depatchify(dec_output.detach().cpu(), n_patches, chw)[0].squeeze())\n",
    "#TensorToImageGrid(depatchify(dec_output.detach().cpu(), n_patches, chw), rows=16, cols=8)\n",
    "TensorToImageGrid(depatchify(patches.squeeze(1), n_patches, chw), rows=8, cols=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorToImage(image_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorToImage(images[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = VisualTransformer(\n",
    "        (1, 28, 28), n_patches=7, n_blocks=2, hidden_d=8, n_heads=2, out_d=10\n",
    "    ).to(device)\n",
    "    N_EPOCHS = 5\n",
    "    LR = 0.005\n",
    "\n",
    "    # Training loop\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    for epoch in trange(N_EPOCHS, desc=\"Training\"):\n",
    "        train_loss = 0.0\n",
    "        for batch in tqdm(\n",
    "            train_loader, desc=f\"Epoch {epoch + 1} in training\", leave=False\n",
    "        ):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "\n",
    "            train_loss += loss.detach().cpu().item() / len(train_loader)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{N_EPOCHS} loss: {train_loss:.2f}\")\n",
    "\n",
    "    # Test loop\n",
    "    with torch.no_grad():\n",
    "        correct, total = 0, 0\n",
    "        test_loss = 0.0\n",
    "        for batch in tqdm(test_loader, desc=\"Testing\"):\n",
    "            x, y = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            y_hat = model(x)\n",
    "            loss = criterion(y_hat, y)\n",
    "            test_loss += loss.detach().cpu().item() / len(test_loader)\n",
    "\n",
    "            correct += torch.sum(torch.argmax(y_hat, dim=1) == y).detach().cpu().item()\n",
    "            total += len(x)\n",
    "        print(f\"Test loss: {test_loss:.2f}\")\n",
    "        print(f\"Test accuracy: {correct / total * 100:.2f}%\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = PointCloudEncoder(latent_dim=LATENT_DIM, num_point=NUM_POINTS).to(DEVICE)\n",
    "#encoder = PointNetBackbone(num_points=NUM_POINTS, num_global_feats=LATENT_DIM, local_feat=False).to(DEVICE)\n",
    "decoder = PointCloudDecoderMLP(latent_dim=LATENT_DIM, num_hidden=3, num_point=NUM_POINTS).to(DEVICE)\n",
    "autoencoder = AutoEncoder(encoder, decoder, device=DEVICE, latent_dim=LATENT_DIM).to(DEVICE)\n",
    "vae = VAE(encoder, decoder, device=DEVICE, latent_dim=LATENT_DIM).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.train import Trainer\n",
    "\n",
    "model_run = Trainer(model=vae, \n",
    "                    criterion=ChamferDistanceLoss(),\n",
    "                    optimizer=optim.Adam(vae.parameters(), config['trainer_parameters']['lr']),\n",
    "                    **config['model_parameters']\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, _, _ = model_run._sanitizer(next(iter(train_loader)))\n",
    "PointsTo3DShape(points[0][0].squeeze())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = points.to(config['model_parameters']['device'])\n",
    "global_features, _, _ = model_run.model.encoder(points)\n",
    "mu = model_run.model.fc_mu(global_features)\n",
    "logvar = model_run.model.fc_logvar(global_features)\n",
    "mu = torch.randn((10000, 128))\n",
    "logvar = torch.randn((10000, 128))\n",
    "\n",
    "#mu = torch.zeros_like(mu)\n",
    "#logvar = torch.zeros_like(logvar)\n",
    "print(mu.mean())\n",
    "print(logvar.mean())\n",
    "\n",
    "model_run.model.reparameterize(mu, logvar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(mu[0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mean(-0.5 * torch.sum(1 + logvar - mu ** 2 - logvar.exp(), dim = 1), dim = 0)\n",
    "# sum(p[i] * log2(p[i]/q[i]) for i in range(len(p)))\n",
    "# KL(P || Q) =  sum x in X P(x) * log(Q(x) / P(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_run.fit(train_loader, val_loader, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END OF TRAINING CYCLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluate = Evaluater(\n",
    "        model=vae,\n",
    "        criterion=ChamferDistanceLoss(),\n",
    "        encoder_type='ConvolutionEncoder',\n",
    "        model_type='VAE',\n",
    "        checkpoint='../models/checkpoints/trial/checkpoint_499.pth',\n",
    "        device='mps',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points, reconstructed_x = model_evaluate.evaluate_data(next(iter(val_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=30\n",
    "PointsTo3DShape(points[N].squeeze().cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PointsTo3DShape(reconstructed_x[N].cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyvista as pv\n",
    "point_cloud = pv.PolyData(reconstructed_x[N].cpu().numpy())\n",
    "mesh = point_cloud.reconstruct_surface()\n",
    "mesh.plot(color='orange')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3515861ec4313dacaa20b0eec5bf326e6557b6589b7b6a4fe3c8baa566747d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
