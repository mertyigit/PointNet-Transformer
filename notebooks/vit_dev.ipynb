{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visual Transformer 2D**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "from glob import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassMatthewsCorrCoef\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.nn.functional import kl_div\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision.datasets.mnist import MNIST\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import open3d as o3\n",
    "import math\n",
    "import yaml\n",
    "import argparse\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "\n",
    "\n",
    "sys.path.append('../')\n",
    "from src.models.VisualTransformerEncoder import *\n",
    "from src.models.VisualTransformerDecoder import *\n",
    "from src.models.MultiHeadAttentionBlock import *\n",
    "from src.models.VisualTransformerGenerator import *\n",
    "from src.utils import features, utils\n",
    "from src.data.dataset import DataMNIST\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP for supressing pytorch user warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is build: True\n",
      "MPS Availability: True\n",
      "Device is set to :mps\n"
     ]
    }
   ],
   "source": [
    "print('MPS is build: {}'.format(torch.backends.mps.is_built()))\n",
    "print('MPS Availability: {}'.format(torch.backends.mps.is_available()))\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print('Device is set to :{}'.format(DEVICE))\n",
    "torch.seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../config/config.yaml', 'r') as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DataMNIST(**config[\"data_parameters\"])\n",
    "data.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = data.train_dataloader()\n",
    "val_dataloader = data.val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PRE & POST PROCESSING SCRIPTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def patchify(images, n_patches):\n",
    "#    n, c, h, w = images.shape\n",
    "#\n",
    "#    assert h == w, \"Patchify method is implemented for square images only\"\n",
    "#\n",
    "#    patches = torch.zeros(n, n_patches**2, h * w * c // n_patches**2)\n",
    "#    patch_size = h // n_patches\n",
    "#\n",
    "#    for idx, image in enumerate(images):\n",
    "#        for i in range(n_patches):\n",
    "#            for j in range(n_patches):\n",
    "#                patch = image[\n",
    "#                    :,\n",
    "#                    i * patch_size : (i + 1) * patch_size,\n",
    "#                    j * patch_size : (j + 1) * patch_size,\n",
    "#                ]\n",
    "#                patches[idx, i * n_patches + j] = patch.flatten()\n",
    "#    return patches\n",
    "#\n",
    "#def depatchify(patches, n_patches, chw):\n",
    "#    # size of patches is expected to be n, h, w\n",
    "#    patch_h = chw[1] // n_patches\n",
    "#    patch_w = chw[2] // n_patches\n",
    "#    n = patches.shape[0]\n",
    "#\n",
    "#    images_recovered = torch.zeros(n, chw[0], chw[1], chw[2],  device='mps')\n",
    "#\n",
    "#    for idx, patch in enumerate(patches):\n",
    "#        \n",
    "#        image_r = torch.empty(0, chw[2],  device='mps')\n",
    "#        \n",
    "#        for i in range(n_patches):\n",
    "#            #patch_r_i_1 = patch[i, :].view(patch_h, patch_w)\n",
    "#            row_tensor = torch.empty(patch_h, 0,  device='mps')\n",
    "#            for j in range(n_patches):\n",
    "#                patch_r_row = patch[i*n_patches+j, :].view(patch_h, patch_w)\n",
    "#                row_tensor = torch.cat((row_tensor, patch_r_row), dim=1)\n",
    "#            \n",
    "#            image_r = torch.cat((image_r, row_tensor), dim=0)\n",
    "#\n",
    "#        images_recovered[idx] = image_r\n",
    "#    return images_recovered\n",
    "#\n",
    "#def get_positional_embeddings(sequence_length, d):\n",
    "#    result = torch.ones(sequence_length, d)\n",
    "#    for i in range(sequence_length):\n",
    "#        for j in range(d):\n",
    "#            result[i][j] = (\n",
    "#                np.sin(i / (10000 ** (j / d)))\n",
    "#                if j % 2 == 0\n",
    "#                else np.cos(i / (10000 ** ((j - 1) / d)))\n",
    "#            )\n",
    "#    return result\n",
    "\n",
    "def TensorToImageGrid(images_batch, rows, cols):\n",
    "    grid = torchvision.utils.make_grid(images_batch, nrow=cols)\n",
    "    grid = grid.permute(1, 2, 0).cpu().numpy()\n",
    "    plt.figure(figsize=(cols, rows))\n",
    "    plt.imshow(grid, cmap='gray')\n",
    "    plt.axis('off')\n",
    "    return plt.show()\n",
    "\n",
    "def TensorToImage(image):\n",
    "    plt.imshow(image.numpy(), cmap='gray')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class SelfAttentionLayer(nn.Module):\n",
    "#    '''\n",
    "#    hidden_d: number of hidden dimensions\n",
    "#    n_heads: number of attention heads\n",
    "#    \n",
    "#    '''\n",
    "#    def __init__(self, hidden_d, n_heads=2):\n",
    "#        super(SelfAttentionLayer, self).__init__()\n",
    "#        self.hidden_d = hidden_d\n",
    "#        self.n_heads = n_heads\n",
    "#\n",
    "#        assert hidden_d % n_heads == 0, f\"{hidden_d} cannot be divided into {n_heads} heads!\"\n",
    "#\n",
    "#        d_head = int(hidden_d / n_heads) ## Hidded dimensions per head\n",
    "#        \n",
    "#        # Define the linear transformation layers for Query, Key, and Value\n",
    "#        self.q_mappings = nn.ModuleList(\n",
    "#            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "#        )\n",
    "#        self.k_mappings = nn.ModuleList(\n",
    "#            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "#        )\n",
    "#        self.v_mappings = nn.ModuleList(\n",
    "#            [nn.Linear(d_head, d_head) for _ in range(self.n_heads)]\n",
    "#        )\n",
    "#        self.d_head = d_head\n",
    "#        self.softmax = nn.Softmax(dim=-1)\n",
    "#\n",
    "#    def forward(self, sequences):\n",
    "#        \n",
    "#        # Sequences has shape (N, seq_length, token_dim)\n",
    "#        # We go into shape    (N, seq_length, n_heads, token_dim / n_heads)\n",
    "#        # And come back to    (N, seq_length, item_dim)  (through concatenation)\n",
    "#\n",
    "#        result = []\n",
    "#        for sequence in sequences:\n",
    "#            seq_result = []\n",
    "#            for head in range(self.n_heads):\n",
    "#                q_mapping = self.q_mappings[head]\n",
    "#                k_mapping = self.k_mappings[head]\n",
    "#                v_mapping = self.v_mappings[head]\n",
    "#\n",
    "#                seq = sequence[:, head * self.d_head : (head + 1) * self.d_head]\n",
    "#                q, k, v = q_mapping(seq), k_mapping(seq), v_mapping(seq)\n",
    "#\n",
    "#                attention = self.softmax(q @ k.T / (self.d_head**0.5)) # Apply the softmax function to calculated the scaled dot-product attention scores to get attention weights\n",
    "#                # attention = F.dropout(attention, p=0.1) # Drop out to attention - optional\n",
    "#                seq_result.append(attention @ v)  # Calculate the weighted sum using the attention weights\n",
    "#            result.append(torch.hstack(seq_result))\n",
    "#        return torch.cat([torch.unsqueeze(r, dim=0) for r in result])\n",
    "#\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class MultiHeadAttention(nn.Module):\n",
    "#    def __init__(self, hidden_d, n_heads):\n",
    "#        super(MultiHeadAttention, self).__init__()\n",
    "#        assert hidden_d % n_heads == 0, f\"{hidden_d} cannot be divided into {n_heads} heads!\"\n",
    "#\n",
    "#        self.hidden_d = hidden_d\n",
    "#        self.n_heads = n_heads\n",
    "#        self.d_k = hidden_d // n_heads ## Hidded dimensions per head\n",
    "#        \n",
    "#        # Define the linear transformation layers for Query, Key, and Value\n",
    "#        self.W_q = nn.Linear(hidden_d, hidden_d)\n",
    "#        self.W_k = nn.Linear(hidden_d, hidden_d)\n",
    "#        self.W_v = nn.Linear(hidden_d, hidden_d)\n",
    "#        self.W_o = nn.Linear(hidden_d, hidden_d)\n",
    "#        \n",
    "#    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "#        # Apply the softmax function to calculated the scaled dot-product attention scores to get attention weights\n",
    "#        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "#        if mask is not None:\n",
    "#            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "#        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
    "#        output = torch.matmul(attn_probs, V) # Calculate the weighted sum using the attention weights\n",
    "#        return output\n",
    "#        \n",
    "#    def split_heads(self, x):\n",
    "#        batch_size, seq_length, hidden_d = x.size()\n",
    "#        return x.view(batch_size, seq_length, self.n_heads, self.d_k).transpose(1, 2)\n",
    "#        \n",
    "#    def combine_heads(self, x):\n",
    "#        batch_size, _, seq_length, d_k = x.size()\n",
    "#        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.hidden_d)\n",
    "#        \n",
    "#    def forward(self, Q, K, V, mask=None):\n",
    "#        Q = self.split_heads(self.W_q(Q))\n",
    "#        K = self.split_heads(self.W_k(K))\n",
    "#        V = self.split_heads(self.W_v(V))\n",
    "#        \n",
    "#        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "#        output = self.W_o(self.combine_heads(attn_output))\n",
    "#        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class PositionWiseFeedForward(nn.Module):\n",
    "#    def __init__(self, hidden_d, d_ff):\n",
    "#        super(PositionWiseFeedForward, self).__init__()\n",
    "#        self.fc1 = nn.Linear(hidden_d, d_ff)\n",
    "#        self.fc2 = nn.Linear(d_ff, hidden_d)\n",
    "#        self.relu = nn.ReLU()\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#class VisualTransformerEncoderBlock(nn.Module):\n",
    "#    '''\n",
    "#    Not typical transformer block, the normalization and linear layers are there but the order is different.\n",
    "#    '''\n",
    "#    def __init__(self, hidden_d, n_heads, mlp_ratio=4):\n",
    "#        super(VisualTransformerEncoderBlock, self).__init__()\n",
    "#        self.hidden_d = hidden_d\n",
    "#        self.n_heads = n_heads\n",
    "#\n",
    "#        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "#        self.mhsa = MultiHeadAttention(hidden_d, n_heads)\n",
    "#        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "#        self.mlp = nn.Sequential(\n",
    "#            nn.Linear(hidden_d, mlp_ratio * hidden_d),\n",
    "#            nn.GELU(),\n",
    "#            nn.Linear(mlp_ratio * hidden_d, hidden_d),\n",
    "#        )\n",
    "#\n",
    "#    def forward(self, x):\n",
    "#        out = x + self.mhsa(self.norm1(x), self.norm1(x), self.norm1(x)) \n",
    "#        out = out + self.mlp(self.norm2(out))\n",
    "#        return out\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class VisualTransformerEncoder(nn.Module):\n",
    "#    def __init__(self, chw, n_patches=7, num_layers=2, hidden_d=8, n_heads=2):\n",
    "#        # Super constructor\n",
    "#        super(VisualTransformerEncoder, self).__init__()\n",
    "#\n",
    "#        # Attributes\n",
    "#        self.chw = chw  # ( C , H , W )\n",
    "#        self.n_patches = n_patches\n",
    "#        self.num_layers = num_layers\n",
    "#        self.n_heads = n_heads\n",
    "#        self.hidden_d = hidden_d\n",
    "#\n",
    "#        # Input and patches sizes\n",
    "#        assert (\n",
    "#            chw[1] % n_patches == 0\n",
    "#        ), \"Input shape not entirely divisible by number of patches\"\n",
    "#        assert (\n",
    "#            chw[2] % n_patches == 0\n",
    "#        ), \"Input shape not entirely divisible by number of patches\"\n",
    "#        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "#\n",
    "#        # 1) Linear mapper\n",
    "#        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "#        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "#\n",
    "#        # 2) Learnable classification token\n",
    "#        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "#\n",
    "#        # 3) Positional embedding\n",
    "#        self.register_buffer(\n",
    "#            \"positional_embeddings\",\n",
    "#            get_positional_embeddings(n_patches**2, hidden_d),\n",
    "#            persistent=False,\n",
    "#        )\n",
    "#\n",
    "#        # 4) Transformer encoder blocks\n",
    "#        self.blocks = nn.ModuleList(\n",
    "#            [VisualTransformerEncoderBlock(hidden_d, n_heads) for _ in range(num_layers)]\n",
    "#        )\n",
    "#\n",
    "#    def forward(self, images):\n",
    "#        # Image to Patches\n",
    "#        n, c, h, w = images.shape\n",
    "#        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "#\n",
    "#        # Patch vector to Hidden Dimensions\n",
    "#        tokens = self.linear_mapper(patches)\n",
    "#\n",
    "#        # Adding classification token to the tokens\n",
    "#        #tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "#\n",
    "#        # Adding positional embedding\n",
    "#        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "#\n",
    "#        # Transformer Blocks\n",
    "#        for block in self.blocks:\n",
    "#            out = block(out)\n",
    "#\n",
    "#        return out, patches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Alt text](image-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class VisualTransformerDecoderBlock(nn.Module):\n",
    "#    def __init__(self, hidden_d, n_heads, d_ff, dropout):\n",
    "#        super(VisualTransformerDecoderBlock, self).__init__()\n",
    "#        self.self_attn = MultiHeadAttention(hidden_d, n_heads)\n",
    "#        self.cross_attn = MultiHeadAttention(hidden_d, n_heads)\n",
    "#        self.feed_forward = PositionWiseFeedForward(hidden_d, d_ff)\n",
    "#        self.norm1 = nn.LayerNorm(hidden_d)\n",
    "#        self.norm2 = nn.LayerNorm(hidden_d)\n",
    "#        self.norm3 = nn.LayerNorm(hidden_d)\n",
    "#        self.dropout = nn.Dropout(dropout)\n",
    "#        \n",
    "#    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "#        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "#        x = self.norm1(x + self.dropout(attn_output))\n",
    "#        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "#        x = self.norm2(x + self.dropout(attn_output))\n",
    "#        ff_output = self.feed_forward(x)\n",
    "#        x = self.norm3(x + self.dropout(ff_output))\n",
    "#        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class VisualTransformerDecoder(nn.Module):\n",
    "#    def __init__(self, chw, d_ff, dropout, n_patches=7, num_layers=2, hidden_d=8, n_heads=2):\n",
    "#        # Super constructor\n",
    "#        super(VisualTransformerDecoder, self).__init__()\n",
    "#\n",
    "#        # Attributes\n",
    "#        self.chw = chw  # ( C , H , W )\n",
    "#        self.n_patches = n_patches\n",
    "#        self.num_layers = num_layers\n",
    "#        self.n_heads = n_heads\n",
    "#        self.hidden_d = hidden_d\n",
    "#        self.d_ff = d_ff\n",
    "#        self.dropout = dropout\n",
    "#        # Input and patches sizes\n",
    "#        assert (\n",
    "#            chw[1] % n_patches == 0\n",
    "#        ), \"Input shape not entirely divisible by number of patches\"\n",
    "#        assert (\n",
    "#            chw[2] % n_patches == 0\n",
    "#        ), \"Input shape not entirely divisible by number of patches\"\n",
    "#        self.patch_size = (chw[1] / n_patches, chw[2] / n_patches)\n",
    "#\n",
    "#        # 1) Linear mapper\n",
    "#        self.input_d = int(chw[0] * self.patch_size[0] * self.patch_size[1])\n",
    "#        self.linear_mapper = nn.Linear(self.input_d, self.hidden_d)\n",
    "#\n",
    "#        # 2) Learnable classification token\n",
    "#        self.class_token = nn.Parameter(torch.rand(1, self.hidden_d))\n",
    "#\n",
    "#        # 3) Positional embedding\n",
    "#        self.register_buffer(\n",
    "#            \"positional_embeddings\",\n",
    "#            get_positional_embeddings(n_patches**2, hidden_d),\n",
    "#            persistent=False,\n",
    "#        )\n",
    "#\n",
    "#        # 4) Transformer encoder blocks\n",
    "#        self.blocks = nn.ModuleList(\n",
    "#            [VisualTransformerDecoderBlock(hidden_d, n_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "#        )\n",
    "#\n",
    "#        self.linear_decoder = nn.Linear(self.hidden_d, self.input_d)\n",
    "#\n",
    "#    def forward(self, images,  enc_output, src_mask, tgt_mask):\n",
    "#        # Image to Patches\n",
    "#        n, c, h, w = images.shape\n",
    "#        patches = patchify(images, self.n_patches).to(self.positional_embeddings.device)\n",
    "#\n",
    "#        # Patch vector to Hidden Dimensions\n",
    "#        tokens = self.linear_mapper(patches)\n",
    "#\n",
    "#        # Adding classification token to the tokens\n",
    "#        #tokens = torch.cat((self.class_token.expand(n, 1, -1), tokens), dim=1)\n",
    "#\n",
    "#        # Adding positional embedding\n",
    "#        out = tokens + self.positional_embeddings.repeat(n, 1, 1)\n",
    "#\n",
    "#        # Transformer Blocks\n",
    "#        for block in self.blocks:\n",
    "#            out = block(out, enc_output, src_mask, tgt_mask)\n",
    "#        \n",
    "#        out = self.linear_decoder(out)\n",
    "#\n",
    "#        images = depatchify(out, self.n_patches, self.chw).to(self.positional_embeddings.device)\n",
    "#\n",
    "#        return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Transformer(nn.Module):\n",
    "#    def __init__(self, hidden_d, n_heads, num_layers, d_ff, dropout, n_patches):\n",
    "#        super(Transformer, self).__init__()\n",
    "#\n",
    "#        self.n_heads = n_heads\n",
    "#        self.hidden_d = hidden_d\n",
    "#        self.num_layers = num_layers\n",
    "#        self.d_ff = d_ff\n",
    "#        self.dropout = dropout\n",
    "#        self.n_patches = n_patches\n",
    "#\n",
    "#        self.encoder = VisualTransformerEncoder((1, 28, 28), n_patches=self.n_patches, num_layers=self.num_layers, hidden_d=self.hidden_d, n_heads=self.n_heads)\n",
    "#        self.decoder = VisualTransformerDecoder((1, 28, 28), d_ff=self.d_ff, dropout=self.dropout, n_patches=self.n_patches, num_layers=self.num_layers, hidden_d=self.hidden_d, n_heads=self.n_heads)\n",
    "#\n",
    "#        \n",
    "#    def generate_mask(self, patches):\n",
    "#        src = torch.ones((patches.size(0), patches.size(1)))\n",
    "#        tgt = torch.ones((patches.size(0), patches.size(1)))\n",
    "#\n",
    "#        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "#        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "#        seq_length = tgt.size(1)\n",
    "#        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "#        tgt_mask = tgt_mask & nopeak_mask\n",
    "#\n",
    "#        return src_mask, tgt_mask\n",
    "#\n",
    "#    def forward(self, images):\n",
    "#        enc_output, patches = self.encoder(images)\n",
    "#        src_mask, tgt_mask = self.generate_mask(patches)\n",
    "#        src_mask = src_mask.to(device='mps')\n",
    "#        tgt_mask = tgt_mask.to(device='mps')\n",
    "#        dec_output = self.decoder(images, enc_output, src_mask, tgt_mask)\n",
    "#        return dec_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_patches = 7\n",
    "#num_layers = 4\n",
    "#hidden_d = 64\n",
    "#n_heads = 8\n",
    "#d_ff = 512\n",
    "#dropout = 0.1\n",
    "#\n",
    "#\n",
    "#encoder = VisualTransformerEncoder((1, 28, 28), n_patches=n_patches, num_layers=num_layers, hidden_d=hidden_d, n_heads=n_heads).to(DEVICE)\n",
    "###decoder_layers = nn.ModuleList([VisualTransformerDecoderBlock(hidden_d, n_heads, d_ff, dropout) for _ in range(num_layers)])\n",
    "#decoder = VisualTransformerDecoder((1, 28, 28), d_ff=d_ff, dropout=dropout, n_patches=n_patches, num_layers=num_layers, hidden_d=hidden_d, n_heads=n_heads).to(DEVICE)\n",
    "#\n",
    "#\n",
    "#images, classes = next(iter(train_loader))\n",
    "#print(images.shape, classes.shape)\n",
    "#patches = patchify(images, n_patches).unsqueeze(1)\n",
    "#print(patches.shape)\n",
    "#\n",
    "#\n",
    "#src_mask, tgt_mask = generate_mask(patches)\n",
    "#src_mask = src_mask.to(DEVICE)\n",
    "#tgt_mask = tgt_mask.to(DEVICE)\n",
    "##src_embedded = positional_encoding(encoder_embedding(src))\n",
    "##tgt_embedded = positional_encoding(decoder_embedding(tgt))\n",
    "##enc_output = src_embedded\n",
    "#enc_output = encoder(images)\n",
    "#print('Encoded Latent Shape: {}'.format(enc_output.shape))\n",
    "#dec_output = decoder(images, enc_output, src_mask, tgt_mask)\n",
    "#print('Decoded Sequence Shape: {}'.format(dec_output.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_patches = 7\n",
    "num_layers = 4\n",
    "hidden_d = 64\n",
    "n_heads = 8\n",
    "d_ff = 512\n",
    "dropout = 0.1\n",
    "learning_rate = 0.00001\n",
    "\n",
    "transformer = Transformer(hidden_d, n_heads, num_layers, d_ff, dropout, n_patches).to(DEVICE)\n",
    "criterion = nn.MSELoss().to(DEVICE)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=learning_rate, eps=1e-9)\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(2):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        images, _ = batch\n",
    "        images = images.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = transformer(images)\n",
    "        loss = criterion(images, output)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "\n",
    "    \n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = next(iter(val_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer.eval()\n",
    "N=17\n",
    "TensorToImage(transformer(images[0])[N].squeeze().detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorToImage(images[0][N].squeeze().detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TensorToImage(depatchify(dec_output.detach().cpu(), n_patches, chw)[0].squeeze())\n",
    "#TensorToImageGrid(depatchify(dec_output.detach().cpu(), n_patches, chw), rows=16, cols=8)\n",
    "TensorToImageGrid(depatchify(patches.squeeze(1), n_patches, chw), rows=8, cols=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorToImage(image_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorToImage(images[0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c3515861ec4313dacaa20b0eec5bf326e6557b6589b7b6a4fe3c8baa566747d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
